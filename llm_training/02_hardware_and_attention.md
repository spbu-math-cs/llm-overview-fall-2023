## Intro to Hardware and optimization

Инженерные оптимизации -- важная часть тренировочного процесса. Во-первых, из-за большого веса LLM не укладывается на одно вычислительное устройство. Во-вторых, тренировка больших моделей -- это долго и дорого (training cost для GPT-4 превосходил 100M$ согласно Sam Altman).

### CPU vs GPU
Главный вычислительный юнит в DL – это GPU. Есть и альтернативы, например, Google использует TPU, разработанный специально для нейросетей. Однако это скорее исключение, и, кроме того, по основным принципам устройства схожи.
<p float="left">
  <img src="assets/CPU.png" height="179">
  <img src="assets/GPU.png" width="200">
</p>

GPU отличается от CPU в основном тем, что вместо небольшого количества быстрых и универсальных ядер у хорошей GPU в распоряжении тысячи Streaming Multiprocessors (SM). Каждый из SM заточен на параллельное исполнение простых операций над частью данных (например, над блоком матрицы). В одном SM обычно сидит 32 Single Processors (SP) (обозначенных на скрине как Cores, хотя, строго говоря, Core -- это скорее SM), они получают одну общую простую инструкцию at a time и исполняют ее каждый над своей частичкой данных (например, над одним числом из того блока матрицы, которым занимается их SM).

GPU получает преимущество над CPU потому, что forward и backward pass хорошо распараллеливаются на группы простых вычислений. То есть не составляет большого труда воспользоваться десятками тысяч SP одновременно, в отличие от большинства не-ML-ных задач. И увеличить таким образом мощность по сравнению с CPU. А то ограничение, что ядро GPU не может произвести все разнообразие инструкций CPU, для тренировочного процесса не принципиально.

### GPU architecture
SP внутри одного SM очень зависимы – они исполняют одновременно одни и те же инструкции и имеют доступ к общему быстрому куску памяти. Однако между SP из разных SM синхронизация обычно не предполагается -- ради эффективности параллелизации. Поэтому нужно выстраивать вычисления так, чтобы инструкции, исполняемые одним SM, не зависели от действий другого -- одна иллюстрация, почему архитектура GPU подходит не для всех задач. С другой стороны, forward pass через Self-Attention легко делится на независимые вычисления по батчу и по разным Attention Heads. Для большей эффективности можно распараллелить и одну голову на одной точке батча, см., например, [FlashAttention-2](https://arxiv.org/pdf/2307.08691.pdf).
<img src="assets/GPU_architecture.png" width='400'>

GPU состоит не только из ядер, внутри есть много другого железа, в частности, своя иерархия памяти. Более глобальные уровни иерархии имеют большее хранилище, но медленную скорость доступа. 
Тут хочется отметить две особенности, к которым мы вернемся. Во-первых, самая длительная операция при вычислениях на GPU – чтение/запись в память, особенно обращения к GPU Global Memory (также называемой High Bandwidth Memory -- HBM), поэтому хочется избегать их любой ценой. <a name="SM_load"></a> Во-вторых, как и ядра CPU, каждый SM можно натравливать сразу на несколько задач, чтобы он мог заниматься чем-то полезным пока ждет, когда к нему из HBM или L2-кэша доедет, скажем, часть матрицы, необходимая для какой-то задачи. То есть если SM приписано мало задач, появляется риск, что SM будет простаивать и средняя мощность просядет.

### What all that has to do with us?
Выше кратко описана целая архитектура со своими ограничениями и сильными сторонами. Под нее пишутся специальные программы, называемые кернелами. Когда я пишу в torch перемножение тензоров или иную популярную операцию, она транслируется в код кернелов, написанный разработчиками. Обычно кернелы для отдельных операций реализованы относительно оптимально. Но когда я запускаю большой алгоритм, состоящий из множества последовательных операций, последовательное независимое исполнение кернелов создает разного рода оверхеды. Например, поскольку кернелы очищают за собой быструю память, следующим кернелам может понадобиться подгрузить те же данные заново из медленной памяти.
Поэтому для ускорения алгоритма можно переписать последовательность кернелов в один длинный кернел, который знает все про алгоритм и засчет этого может срезать углы. Эта концепция называется <a name="Fusion"></a> Kernel Fusion. К ней мы еще вернемся, а пока ниже приведена иллюстрация как имплементировать собственный кернел в торче. Цитата из документации:

"_The general strategy for writing a CUDA extension is to first write a C++ file which defines the functions that will be called from Python, and binds those functions to Python with pybind11. Furthermore, this file will also declare functions that are defined in CUDA (.cu) files_":

<figure>
    <figcaption>CUDA kernel code (.cu) </figcaption>
    <img src="assets/kernel1.png" width='400'>
  </figure>
  
<figure>
    <figcaption>.py script to compile C++ and use 'lltm' directly from Python</figcaption>
    <img src="assets/kernel3.png" width='400'>
  </figure>

Кроме переписывания кернелов алгоритмы можно делать менее интенсивными за счет разного рода аппроксимаций -- например, mixed precision или с помощью разнообразных приближенных алгоритмов вроде [Linformer](https://arxiv.org/abs/2006.04768). Однако они приводят к потере качества и часто концентрируются на computational complexity вместо увеличения wall-clock speed на практике, из-за чего в основном не получили широкого применения.

### Attention: "computational" bottleneck
Грубо говоря, все LLM сегодня используют Attention механизм. При этом Attention – главный ботлнек при масштабировании на последовательности большой длины. Причина в подсчете матрицы весов для перевзвешивания Values, растущей квадратично по длине последовательности.

<a name="Atten_notation"></a>

<img src="assets/Attention_notation.png" width="800">

Таким образом, значительная часть оптимизации LLM -- это оптимизация Attention-слоя. 

### Flash-Attention
Хотя с первого взгляда таинственной и озадачивающей, однако вполне правдоподобной исходя из общей логики пайплайна развития ML смотрится дата 2022 года на первой странице [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf).


Суть статьи -- оптимизация Attention-слоя с помощью [Kernel Fusion](#Fusion). Wall-clock speedup в тот момент составил x3.5 для GPT-2 по сравнению с имплементацией [Huggingface](https://huggingface.co/docs/hub/index), а дополнительная память уменьшилась с квадратичной до линейной по N в терминах [нотации выше](#Atten_notation). 
Flash-Attention (FA) можно воспринимать как минимально разумную имплементацию механизма Attention на GPU. По сути FA -- это два кернела, один для подсчета forward pass через Attention слой, второй -- для подсчета backward pass. Основной инсайд авторов состоит в том, что ботлнек наивной имплементации Attention -- это не объем вычислений, а объем памяти больших промежуточных матриц, переезжающих то в кэш SM, то обратно в глобальную память GPU по несколько раз (матрицы S и P).
Далее я буду строить рассуждения в предположении N>>d.

Вычислений на одном проходе происходит немного -- O(N^2), поскольку все операции либо element-wise, либо состоят из перемножения небольших матриц Nxd. На этом фоне обмен памяти порядка N^2 очень значителен, ведь самая тяжелая часть вычислений на GPU -- это доступы к памяти. Ситуация усугубляется при добавлении masking и dropout -- последовательное выполнение этих операций разными кернелами также предполагает загрузку матриц NxN из глобальной памяти, выполнение O(N^2) операций, и выгрузку обновленной матрицы обратно.

Исходя из этих соображений авторы написали реализацию одним кернелом, проходящим через весь Attention-слой разом, без необходимости "запоминать", а затем "вспоминать" большие промежуточные матрицы. Если абстрактно, то ценой небольшого увеличения [FLOPs](https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning) можно перевзвешивать Values с помощью матрицы вероятностей P поступательно, не удерживая большие матрицы в памяти целиком. Алгоритм (не страшный) можно посмотреть в [статье](https://arxiv.org/pdf/2205.14135.pdf).

<img src="assets/FA_image.png" width="600">

### Flash-Attention 2
FA2 -- логичное развитие FA.
В статье про FA внимание уделялось не столько [загруке SM](#SM_load) и обеспечению высокой производительности GPU, сколько уменьшению больших потоков памяти, текущих между слоями иерархии памяти GPU. 
В результате проблему с памятью удалось нивелировать, и вопрос загрузки SM стал острее. Авторы обнаружили, что в текущем варианте имплементации невысока occupancy и присутствуют конфликты памяти, из-за которых одним потокам приходится ждать другие. Решение -- увеличение параллелизации (для загрузки SM) и небольшое перетасовывание задач между SM, чтобы разные SM не лезли в общие куски глобальной памяти. Как устроена эта оптимизаци можно найти в небольшой [cтатье](https://arxiv.org/pdf/2307.08691.pdf).

### Single GPU is not enough though
Вернемся к первой причине этого разговора -- большая модель не влезает на одну видеокарту! Поэтому придется класть ее сразу на несколько.

<img src="assets/node.png" width="500">

## Коротко о примитивах для обмена данными между GPU (автор: Георгий Ангени)

В алгоритмах, описанных в следующих частях будут появляться такие термины, как AllReduce, ReduceScatter, AllGather. Это названия операций из библиотеки NCCL (NVIDIA Collective Communication Library), в которой реализованы примитивы коммуникации между графическими устройствами NVIDIA (и оптимизированы для достижения наибольшей пропускной способности, а также обеспечения низкой задержки). Подробнее о примитивах, реализованных в библиотекте, можно изучить [здесь](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html).

## Сравнение скоростных показателей на примере DGX-2 (автор: Георгий Ангени)

В качестве источника взят [datasheet](https://www.nvidia.com/content/dam/en-zz/es_em/Solutions/Data-Center/dgx-2/nvidia-dgx-2-datasheet.pdf) с официального сайта NVIDIA.

* в состав системы входит 16 NVIDIA Tesla V100 с суммарной проивзодительностью в 2 petaFLOPS с суммарным объемом видеопамяти в 512GB
* суммарная пропускная способность (т.н. [bisection bandwidth](https://en.wikipedia.org/wiki/Bisection_bandwidth)) между графическими ускорителями, благодаря технологии [NVLink](https://www.nvidia.com/ru-ru/data-center/nvlink/), равна 2.4TB/sec (одно соединение имеет скорость 300GB/sec)
* связь внутри кластера осуществляется с помощью 8 Infiniband соединений с пропускной способностью 100GB/sec в одну сторону
* связь по сети осуществляется с помощью высокоскоростного Ethernet-адаптера с пропускной способностью до 100GB/sec
* также в состав системы входит
    * 24-ядерный процессор с тактовой частотой 2.7Ghz
    * 1.5TB оперативной памяти (конкретная частота и модель не указана, но для сравнения память DDR4-3200 имеет пропускную способность в 25.6GB/sec)
    * около 30TB постоянной памяти (используются NVME SSD, пропускная способность которых обычно составляет несколько GB/sec)

Отдельно стоит упомянуть скорости, присущие самим вычислительным устройствам, а не системам с ними. Обычно подобные значения определяются опытным путем с помощью [микробенчмаркинга](https://arxiv.org/pdf/1804.06826.pdf). Говоря о Tesla V100, ее производительность равна 125 teraFLOPS, а также
* 2155 GB/sec – пропускная способность L2 кэша
* ~800 GB/sec - пропускная способность global memory
