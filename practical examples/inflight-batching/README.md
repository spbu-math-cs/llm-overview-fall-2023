## Пререквизиты

```
conda create --name=llama2
conda activate llama2
conda install python=3.8
pip install -r requirements.txt
```

Для запуска бенчмарка без inflight batching вводим `python3 inflight_bench_baseline.py`

Для запуска бенчмарка с использованием inflight batching вводим `python3 inflight_bench.py`

При реализации inflight batching был создан самописный критерий остановки (`StoppingCriteria`), а также для тестирования времени инференса модифицированы некоторые функции `GenerationMixin` (`generate`, `sample`)

## Результаты сравнения

Примитивный батчинг и inflight batching были сравнены на продолжении набора из `test_size` строк `Fun fact of the day: ` с варированием размера батча и максимальной длины генерации. Результаты сравнения средней пропускной способности (среднее и среднеквадратичное отклонение) представлены в таблицах ниже

|max_length|30 tokens|40 tokens|50 tokens|
|---|---|---|---|
|Baseline|1.6824 ± 0.0403|**1.3557 ± 0.1941**|**1.275 ± 0.04**   |
|Inflight|1.452 ± 0.104  |**1.4981 ± 0.1708**|**1.5466 ± 0.0487**|

*В таблице выше представлены результаты для `batch_size=8` и `test_size=32`*

|batch_size|4 queries|8 queries|16 queries|
|---|---|---|---|
|Baseline|1.5548 ± 0.0975|**1.2423 ± 0.04**  |**1.9020 ± 0.2815**|
|Inflight|1.4370 ± 0.1281|**1.7200 ± 0.0487**|**2.2131 ± 0.2234**|

*В таблице выше представлены результаты для `max_length=50` и `test_size=64`*

С увеличением максимальной длины генерируемой последовательности и размера батча увеличивается и потенциальная разница в возможных длинах последовательностей, которые модель будет генерировать внутри батча на самом деле. В данной ситуации inflight batching помогает, поскольку он позволяет коротким генерациям не дожидаться, пока закончатся долгие генерации, а вместо этого генерировать продолжения новых последовательностей.

Несмотря на то, что удалось установить наличие сценариев, при которых inflight batching показывает себя лучше, чем примитивная реализация, предложеная реализация inflight batching является наивной и остается место для ее оптимизаций.

Так, например, при каждом вызове функции `generate()` (см. класс `GenerationMixin`) лежащая в основе функция авторегрессионной генерации (например, `sample()`) страдает от проблемы долгой генерации первого токена, поскольку KV-кэш при каждом вызове `generate()` не сохраняется, хотя в идеальном случае (поскольку генерации для параллельных запросов не влияют друг на друга) можно сбрасывать кэш исключительно для тех последовательностей, генерация которых завершилась и на место которых были загружены новые запросы.

При этом при примитвном батчинге функция `generate()` вызывается $\mathcal{O}(test\_size / batch\_size)$ раз, а для текущей реализации inflight batching она вызывается $\mathcal{O}(test\_size)$ раз.
