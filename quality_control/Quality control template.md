# Оценка качества

# Введение

- Точечные оценки
- Графические методы
- Энтропия
- Перплексия

В каждой задаче машинного обучения важна оценка результатов моделей. Более того, важно учесть правильный выбор метрик для поставленной задачи.

Для досконального и качественного изучения оценок качества рассмотрим, прежде всего, бинарную классификацию.

Для классификаций данного рода имеется 4 вида исходов: истинно положительные (TP), истинно отрицательные (TN), ложно положительные (FP) и ложно отрицательные (FN). В таких случаях можно применять следующие метрики:

*в форме точечных оценок*:

1. ****Accuracy**** (Точность), которая показывает долю правильно проставленных меток класса от общего количества данных:

**Accuracy** = (TP + TN)/(TP + TN + FP + FN) 

Рассмотрим пример: мы хотим оценить работу спам-фильтра электронной почты. У нас есть 100 не-спам писем, 90 из которых наш классификатор определил верно (то есть TN = 90, FP = 10), и 10 спам-писем, 5 из которых классификатор также определил верно (TP = 5, FN = 5).

В таком случае, Accuracy примет значение:

**Accuracy** = (TP + TN)/(TP + TN + FP + FN) = (5 + 90)/(5 + 90 + 10 + 5) ~ 0.864.

Но если мы просто будем предсказывать все письма как не-спам, то получим более высокий результат:

**Accuracy** = (TP + TN)/(TP + TN + FP + FN) = (0 + 100)/(0 + 100 + 0 + 10) ~ 0.909.

Более того, наша модель совершенно не обладает никакой предсказательной силой, так как изначально мы хотели определять письма со спамом.

1. ****Precision**** (как ни странно, тоже Точность), которая показывает долю истинно положительных исходов от всего набора положительных меток:

****Precision**** = ********TP / (TP + FP)

В случае с примером по спам-фильтру, ****Precision**** = ********TP / (TP + FP) = 5 / (5 + 10) ~ 0.333

1. ****Recall**** (Чувствительность), которая показывает долю положительных среди всех меток класса, которые были определены как «положительный».

****Recall =**** TP / (TP + FN)

В случае с примером по спам-фильтру, ****Recall =**** TP / (TP + FN) = 5 / (5 + 5) = 0.5

1. ****F_1-Score**** (применяется, если Precision и Recall совпадают), которая вычисляет их среднее гармоническое для получения оценки результатов:

****F_1-Score**** = ****(Precision * Recall)**** / (****Precision**** + ****Recall).****

В случае с примером по спам-фильтру, ****F_1-Score**** = ****(Precision * Recall)**** / (****Precision**** + ****Recall)**** = (0.333 * 0.5)/(0.333 + 0.5) ~ 0.2

С точки зрения математической статистики, FP соответствует количеству ошибок I-го рода, а FN - количеству ошибок II-го рода. Более того, FP (как аналог ошибки I-го рода) описывает слабость (иными словами, ***немощность***) критерия, по которому осуществляется бинарная классификация, в то время как FN - слепоту (иными словами, ***нечувствительность***) критерия.

И в случае несбалансированных выборок в метриках по FP и FN будут происходить существенные искажения.

*в форме графических методов*:

Для данных метрик введём вспомогательные переменные - True Positive Rate (TPR) и False Positive Rate (FPR):

TPR = TP /(TP + FN)

FPR = FP /(FP + TN)

1. **ROC**, показывающий зависимость верно классифицируемых объектов положительного класса (TPR) от ложно положительно классифицируемых объектов негативного класса (FPR).
2. ****AUC****, вычисляющая площадь под кривой ROC.

Рассмотрим следующую задачу:

нам необходимо выбрать 100 релевантных документов из 1 миллиона документов. Мы обучили два алгоритма:

**Алгоритм 1** возвращает 100 документов, 90 из которых релевантные.

То есть TPR = 0.9, FPR ~ 0.00001.

**Алгоритм 2** возвращает 2000 документов, 90 из которых релевантные.

То есть TPR = 0.9, FPR ~ 0.00191.

На первый взгляд кажется, что первый алгоритм лучше, т.к. FPR меньше. Однако разница в False Positive Rate между этими двумя алгоритмами очень мала — только 0.0019. Причиной тому является AUC-ROC, который измеряет долю FP относительно TN. И в задачах, где нам не так важен больший класс, может делать некорректные выводы при сравнении алгоритмов.

Поэтому вернёмся к ****Precision**** и ****Recall****:

**Алгоритм 1:**

****Precision**** = 0.9 , ****Recall**** = 0.9

**Алгоритм 2:**

****Precision**** = 0.045, ****Recall**** = 0.9

И именно здесь заметна значительная разница в точности между двумя алгоритмами — 0.855.

Также не стоит забывать и мульти-классификацию, где происходит вычисление среднего метрики по всем классам. Тогда в качестве **«положительного»** класса берется вычисляемый, а все остальные — в качестве **«отрицательного»**.

Помимо метрик стоит упомянуть и такую характеристику как **энтропия,** которая фактически является мерой беспорядка. Чем легче будет распределить объекты на классы, тем ниже энтропия.

Также стоит упомянуть и другие описания энтропии - данная величина является показателем того, как сильно мы можем ***компактизировать*** битовую информацию о сообщении. Говоря более простыми словами, энтропия ограничивает максимально возможное сжатие без потерь (или почти без потерь), которое может быть реализовано при использовании теоретически (то есть, чем проще упростить информацию, тем ниже энтропия).

По формуле Шеннона, энтропия представляется следующим образом:

$H(x) = - sum_i (p_i * log p_i)$

Скобка с логарифмом обозначает измеряемое в битах количество информации, содержащейся в том событии, что случайная величина приняла значение. В то время как H(x) - количество информации, которое в среднем приходится на одно событие.

У энтропии есть значимые преимущества:

1. В отличие от энтропии, метрика Accuracy очень чувствительна к порядку данных в тренировочном наборе, а также не учитывается статистическая достоверность.
2. Энтропия хорошо сочетается с сигмоидой и моделью логистической регрессией, которая занимается распределениями, а не списком вопросов.

Ещё одним важным инструментом в оценке качества моделей машиной обучения является **перплексия** - мера того, насколько хорошо распределение вероятностей предсказывает выборку.

Выражается перплексия следующим образом:

$PP(p) = 2^H, H = H(p)$

Основание логарифма не обязательно должно быть равно 2: перплексия не зависит от основания логарифма при условии, что энтропия и показательная функция имеют одно и то же основание. 

# Оценки без привлечения людей

- **Внутренние:**
    - *Классические оценки*:
        - Перплексия
        - **BLEU (+ NIST)**
        - ROUGE
        - METEOR
    - *Нейросетевые*
        - Что такое эмбеддинги
        - Референсные
        - Безреференсные
        - Сравнение с классическими оценками
    - Бенчмарки на примере SuperGLUE

# Оценки с привлечением людей

- Ранжирование списка моделей по качеству на конкретной задаче / данных
- Pairwise comparison
- Метод Брэдли-Терри

### LLM как эксперт

Огромный потенциал LLM-моделей позволяет им не только решать лучше людей некоторые задачи, но и выступать в роли эксперта для разметки данных.

GPT-3 может использоваться вместо краудсорсинга для задачи разметки текстов [(Ding, 2022)](https://arxiv.org/pdf/2212.10450.pdf). Авторы приходят к выводу, что при ограниченном бюджете на разметку, качество обученных моделей будет не хуже, чем при разметке реальными людьми. 

Также есть исследования ****[(Alizadeh, 2023)](https://arxiv.org/pdf/2307.02179.pdf), что open-source LLM модели показывают сравнимые с ChatGPT результаты, при этом при наличии вычислительных мощностей, могут быть значительно более эффективными инструментами разметки в аспектах стоимости и скорости.

Кроме того, человеческая разметка на деле может оказаться разметкой с помощью LLM моделей, так как краудсорсеры могут использовать LLM-модели для повышения своей производительности. При задаче разметки текста в Amazon MTurk до 33-46% ответов людей могут быть на самом деле ответами LLM-моделей ****[(Veselovsky, 2023)](https://arxiv.org/pdf/2306.07899.pdf). Это может служить мотивацией в большей степени исследовать методы оценки с помощью LLM, так как краудсорсинг может перестать быть разметкой с помощью людей на самом деле.

Например, в бенчмарке [rulm-sbs2](https://github.com/kuk/rulm-sbs2) в качестве эксперта используется GPT-4. И приводится список причин, по которым была выбрана LLM вместо человеческой разметки.

- При ручной проверке 100 случайных пар ответов из оценок толокеров из проекта [Saiga](https://huggingface.co/datasets/IlyaGusev/rulm_human_preferences) оказывается, что в 70 случаях ответы толокеров и GPT-4 совпали, 15 раз скорее были правы краудсорсеры, 15 раз скорее была права GPT-4
- При этом у GPT-4 лучше получается отвечать на вопросы, требующие экспертизы, например, решение задач по программированию или математике

Таблица со сравнением GPT-4 и краудсорсеров из проекта [Toloka](https://toloka.ai) из описания бенчмарка:

|  | Crowd | GPT-4 |
| --- | --- | --- |
| Цена | 67$ за 1000 заданий с перекрытием 5 | 45$ за 1000 заданий |
| Скорость | 1 час модерация проекта + ~250 заданий в час | ~3600 заданий в час |
| Качество | Краудсорсеры ошибаются в ~15% заданий.
Сложно оценить задания про программирование, математику | GPT-4 ошибается в ~15% заданий.
Не может оценить задания, где у самой GPT4 низкое качество: задания на знание русского языка, задания на запретные темы |
| Предвзятость | Краудсорсеры беспристрастны | GPT-4 скорее нравятся ответы GPT-4 и Turbo |
| Интерпретация | Краудсорсеры не объясняют решение | Может объяснить решение |

Можно сделать вывод, что при сравнимом качестве оценка с помощью GPT-4 дешевле и быстрее. Также имеет смысл совмещать оценку с помощью людей и LLM, так как природа ошибок разная, а также необходимо устранять предвзятость LLM в оценке. Также LLM можно использовать для объяснения разницы в примерах, вызывающих неопределенность у толокеров. 

Потенциально могут появляться специализированные LLM или фреймворки для разметки данных и оценки качества. Например, AnnoLLM [(He, 2023)](https://arxiv.org/pdf/2303.16854.pdf) предлагает подход по улучшению процесса разметки данных с помощью ChatGPT.

### Проблемы использования LLM вместо краудсорсинга

- Автоматическая оценка с помощью LLM моделей поддается атакам, которые искажают результаты сравнения [(Wang, 2023)](https://arxiv.org/pdf/2305.17926v1.pdf). Исследованная атака помогала достичь систематической предвзятости за счет перемешивания примеров для оценки внутри контекста LLM модели
- LLM модели отдают большее предпочтение стилю ответов, чем верности фактов [(Gudibande, 2023)](https://arxiv.org/pdf/2305.15717.pdf)
- Текущие фреймворки и датасеты могут не отражать картины, наблюдаемой при реальном использовании модели, поэтому не должны применяться для решения о выводе новых моделей в production использование
- LLM отдают большее предпочтение тем моделям, с которым они делят тренировочный датасет
- Оценка с помощью LLM не проверяет такие особенности, как грубость и токсичность, так что не может служить основанием для внедрения модели в production

### Бенчмарки для оценки качества

- [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval) — фреймворк для автоматической оценки моделей с помощью LLM
    - Большой выбор моделей-оценщиков
    - Leaderboard из 12+ LLM
- [rulm-sbs2](https://github.com/kuk/rulm-sbs2) — бенчмарк для ранжирования LLM по качеству по качеству работы на русском языке
    - GPT-4 как эксперт для оценки
    - 500 заданий из 15+ категорий
    - Задания — переведенные на русский Alpaca + Vicuna + часть Arena